{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络复习\n",
    "1.关键词：全连接网络（线性变换）、偏置、权重\n",
    "2.h =xW +b      矩阵计算\n",
    "3.激活函数（非线性变换）—sigmoid函数\n",
    "y=σ(x)= 1/（ 1 +exp(−x) ）    （呈S形曲线）   求导dy/dx=y*(1-y)\n",
    "接收任意大小的实数，输出0～1的实数\n",
    "\n",
    "4.层的类化和正向传播的实现\n",
    "所有的层都有forward()方法和backward()方法;\n",
    "所有的层都有params和grads实例变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.27727393  0.94250444  0.27045385 -1.04991069]\n",
      " [-1.50235614  0.33290592  0.37814271 -0.98939756]\n",
      " [-1.61001545  0.5739993   0.33462916 -1.28620585]\n",
      " [-1.26986084  1.46912613  0.28018922 -1.23760909]\n",
      " [-1.46356943  0.40602428  0.37242359 -0.97174934]\n",
      " [-1.43134909  0.64290422  0.33382039 -1.06771653]\n",
      " [-1.10950785  0.52350731  0.28363701 -0.63554865]\n",
      " [-1.50721441  0.3172681   0.37794267 -0.99012632]\n",
      " [-1.2179484   1.76240424  0.30559916 -1.25218361]\n",
      " [-1.29472107  0.41630812  0.35035111 -0.76894961]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.Params=[]\n",
    "    def forward(self,x):\n",
    "        out=1/(1+np.exp(-x))\n",
    "        self.out=out\n",
    "        return out\n",
    "    \n",
    "    def backforward(self,dout):\n",
    "        return dout*self.out*(1-self.out)\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.Params=[w,b]\n",
    "        self.x=None\n",
    "        self.grads=[np.zeros_like(w),np.zeros_like(b)]\n",
    "\n",
    "    def forward(self,x):\n",
    "        w,b=self.Params\n",
    "        h=np.dot(x,w)+b\n",
    "        self.x=x\n",
    "        return h\n",
    "\n",
    "    def backward(self,dout):\n",
    "        w,b=self.Params\n",
    "        x=self.x\n",
    "        db=np.sum(dout,axis=0)\n",
    "        dx=np.dot(dout,w.T)\n",
    "        dw=np.dot(x.T,dout)\n",
    "\n",
    "        self.grads[0][...]=dw\n",
    "        self.grads[1][...]=db\n",
    "\n",
    "        return dx\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        I,H,O=input_size,hidden_size,output_size\n",
    "\n",
    "        w1=np.random.randn(I,H)\n",
    "        b1=np.random.randn(H)\n",
    "        w2=np.random.randn(H,O)\n",
    "        b2=np.random.randn(O)\n",
    "\n",
    "        self.layers=[\n",
    "            Affine(w1,b1),\n",
    "            Sigmoid(),\n",
    "            Affine(w2,b2)\n",
    "        ]\n",
    "\n",
    "        self.Params=[]\n",
    "        for layer in self.layers:\n",
    "            self.Params+=layer.Params\n",
    "\n",
    "    def predict(self,x):\n",
    "        for layer in self.layers:\n",
    "            x=layer.forward(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "x=np.random.randn(10,2)\n",
    "model=TwoLayerNet(2,3,4)\n",
    "res=model.predict(x)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.神经网络的学习（反向）\n",
    "损失函数（交叉熵误差cross entropy error），交叉熵误差由神经网络输出的各类别的概率和监督标签求得\n",
    "Softmax层求Softmax 函数的值，用Cross Entropy Error层求交叉熵误差。\n",
    "\n",
    "Softmax层： yk = exp(sk)/ (i=1~n求和)exp(si)\n",
    "Softmax 函数输出的各个元素是0.0～1.0的实数。另外，如果将这些元素全部加起来，则和为1。因此，Softmax的输出可以解释为概率。 \n",
    "\n",
    "Cross Entropy Error层   L =− （k=1~n求和）tk*logyk \n",
    "tk是对应于第k个类别的监督标签（one hot）\n",
    "L =−1/N \\*(n=1~N) (k) tnk*log(ynk)      N笔数据求平均值\n",
    "\n",
    "两层合并为Softmax with Loss层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导数和梯度\n",
    "W和dL/dW 具有相同的形状,矩阵和其梯度具有相同形状!\n",
    "## 链式法则\n",
    "误差反向传播法的关键是链式法则\n",
    "1.加法节点将上游传来的值乘以1，再将该梯度向下游传播。\n",
    "2.乘法节点\n",
    "3.分支节点/repeat节点\n",
    "4.sum节点\n",
    "5.MatMul节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.77826601  1.30531275 -3.344384    1.2860984  -2.04657512  0.61686348\n",
      "  -1.81078717  1.2327498 ]]\n",
      "[[-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]\n",
      " [-0.40568363 -1.09701156  0.20293221  0.61140193 -0.36182797 -0.44542417\n",
      "  -0.91539304 -0.26337051]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#repeat节点\n",
    "D,N=8,7\n",
    "x=np.random.randn(1,D)\n",
    "y=np.repeat(x,N,axis=0)     #指定axis，可以指定沿哪个轴复制。\n",
    "\n",
    "dy=np.random.randn(N,D)     \n",
    "dx=np.sum(dy,axis=0,keepdims=True)      #指定keepdims=True，可以维持二维数组的维数。\n",
    "print(dx)\n",
    "\n",
    "#sum节点\n",
    "x=np.random.randn(N,D)\n",
    "y=np.sum(x,axis=0,keepdims=True)\n",
    "\n",
    "dy=np.random.randn(1,D)\n",
    "dx=np.repeat(dy,N,axis=0)\n",
    "print(dx)\n",
    "#Sum节点和Repeat节点存在逆向关系\n",
    "\n",
    "#matmul节点\n",
    "class Matmul:\n",
    "    def __init__(self,w):\n",
    "        self.Params=[w]\n",
    "        self.x=None\n",
    "        self.grads=[np.zeros_like(w)]\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.x=x\n",
    "        w,=self.Params\n",
    "        out = np.dot(x,w)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        w,=self.Params\n",
    "        x=self.x\n",
    "\n",
    "        dw=np.dot(x.T,dout)\n",
    "        dx=np.dot(dout,w.T)\n",
    "\n",
    "        self.grads[0][...]=[dw] #[...]浅拷贝和深拷贝区别\n",
    "        return dx\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
